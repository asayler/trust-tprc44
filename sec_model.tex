\section{Modeling Trust}
\label{sec:model}

Researchers from a variety of disciplines have proposed a range of
trust definitions and models~\cite{grandison2000, camp2003,
  sabater2005, flowerday2006}. These models range from technical
models for calculating reputations via machine-learning algorithms to
sociological models for exploring legal and social noticing of
trust. In this section, I propose a trust model tailored to the manner
in which user interact with third parties across the modern computing
landscape. In particular, this model aims to provide a basis for
describing how users trust third parties with access to their digital
data and the manner sin which this trust might be violated.

Before defining a model for trust, it is useful to define some of the
relevant terms used in this model. To start, I'll define
\textit{trust} as the expectation that a given entity will behave in a
promised manner. \textit{Violations} of trust thus occur whenever said
entities deviate from this expectation. Trust is closely related to
two other properties inherent in modern computing ecosystems: security
and privacy. Like trust, these terms have wide-ranging meanings across
a variety of disciplines. For the purposes of this discussion, I'll
define \textit{security} as the notion of user control over the
behavior of a given system. A \textit{secure system} is thus a system
that behaves in the manner the user desires. Facets of this notion of
security include \textit{confidentiality}, the ability to control who
can read user data, and \textit{authenticity}, the ability to control
the integrity and verifiable modification of user data. Finally,
confidentiality and authenticity provide for a definition of
\textit{privacy} as the ability to control both the access and
modification of user data as well as the ability to control the
historical record of such access or modification.

When users leverage modern computing devices and services, they must
trust third party manufacturers and service providers to be good
stewards of a variety of digital data, from stored files to location
information to communications. The nature of this trust has two main
factors:

\begin{packed_desc}
\item[Degree:] How much trust must a user place in a third party
  (e.g. what capabilities do they allow a third party to exercises with
  respect to user data)?
\item[Risk:] In what manners can the third party violate this trust
  (e.g. how can the third party abuse the capabilities they have been
  granted)?
\end{packed_desc}

The security and privacy of a user's data is generally dependent on
these two axis: the higher the degree of trust a user places in a
third party, the more power that party has to violate the privacy or
security of a user's data. Similarly, the higher the risk of trust
violations, the higher the risk of a violation of the security or
privacy of a user's data.  Intuitively, the best way to enhance the
privacy of user data is thus to aim to either minimize degree of third
party trust, to minimize risk of third party trust violations, or to
minimize both.

In terms of degree of trust, third parties can be trusted with the
following data-related capabilities:

\begin{packed_desc}
\item[Storage (S):] \hfill \\ Can a third party faithfully store
  private user data and make it available to the user upon request?
  Misuse of this capability may result in a loss of user data, but
  won't necessarily result in the exposure of user data.
\item[Access (R):] \hfill \\ Can a third party read and interpret the
  private user data they store? Misuse of this capability may result
  in the exposure of user data.
\item[Manipulation (W):] \hfill \\ Can a third party modify the
  private user data to which they have access? Misuse of this
  capability may result in the ability to manipulate a user
  (e.g. changing appointments on a user's calendar, etc).
\item[Meta-analysis (M):] \hfill \\ Can a third party gather user
  metadata related to any stored user data or user behavior
  interacting with this data? Misuse of this capability may result in
  the ability to infer private user data (e.g. who a user's friends
  are based on data sharing patterns).
\end{packed_desc}

Trust violation occurs when a third party exercises any of the above
capabilities without explicit user knowledge and permission. Put
another way, a trust violation occurs whenever a third party leverages
a capability with which they are entrusted in a manner in which the
user does not expect the capability to be leveraged.

There are several types of trust violations. Each is defined by the
manner in which the violation occurs and the motivations behind it:

\begin{packed_desc}
\item[Implicit (P):] \hfill \\ This class of trust violation occurs
  when a third party violates a user's trust in a manner approved by
  the third party. An example might be sharing user data with a
  business partner (e.g. an advertiser). Often these forms of
  violations aren't really ``violations'' in the sense that a user may
  have clicked through a Terms of Service agreement that granted
  implicit permission for such use, but if the third party is engaging
  in behavior that the user would not generally expect, an implicit
  trust violation has occurred.
\item[Compelled (C):] \hfill \\ This class of trust violation occurs
  when a third party is compelled by another actor to violate a user's
  trust. The most common example would be a third party being forced
  to turn over user data or records in response to a request from the
  government with jurisdiction over the party. Another example might
  include a company going bankrupt and being forced to sell its user
  data to another entity~\cite{solove2015}.
\item[Unintentional (U):] \hfill \\ This form of violation occurs when
  a third party unintentionally discloses or manipulates user data. An
  example would be a coding error that allows either the loss of or
  unfettered access to user data.
\item[Colluding (L):] \hfill \\ This class of violation occurs when
  multiple third parties collude to gain capabilities over user data
  beyond what the user intended each to have individually.
\end{packed_desc}
