\section{Analysis of Third Party Trust}
\label{sec:analysis}

The trust model proposed in \S~\ref{sec:model} is primarily useful for
what it can tell us about the nature of user trust and the modern
computing landscape. As such, I apply the model to analyze the
capabilities granted to a number of popular third party computing
services.

\subsection{Capability Examples}
\label{sec:analysis:capabilites}

Third party based ``cloud'' computing services have become extremely
popular over the previous 10 years.  The question of how
\textit{trustworthy} these services are is addressed later in
~\S~\ref{sec:analysis:violations}. In this section, I explore how
\textit{trusted} such service are. That is, how much trust must users
place in such services? The capability axis of my proposed model is
useful to quantify this trust.

\subsubsection{File Storage}

Cloud file storage is a popular third party use case. Services such as
Dropbox~\cite{dropbox}, Google Drive~\cite{google-drive}, and
Microsoft OneDrive~\cite{microsoft-onedrive} all provide users with
mechanisms for storing their files in the cloud, often for the purpose
of keeping files synced across multiple devices or to provide the
ability to share files or collaboratively edit them with other
users. Traditional cloud storage services such as Dropbox, Drive, and
OneDrive are similar enough in their operation that I will use Dropbox
as a stand in for the analysis of all three.

What capabilities is a normal Dropbox user entrusting to Dropbox?
Clearly, users must trust Dropbox to faithfully store their data since
that is Dropbox's core purpose. Users therefore grant Dropbox the
\emph{S} capability. Furthermore, users must also grant Dropbox the
ability to read and access their data (i.e. the \emph{R} capability)
in order to support Dropbox's sharing and syncing features. While
Dropbox doesn't generally utilize it, users are also effectively
granting Dropbox the manipulation (\emph{W} capability) as well since
the user has no mechanisms for ensuring that Dropbox can't manipulate
their data. Finally, Dropbox has full access to user metadata related
to their usage of the service, granting them the \emph{M}
capability. Therefore, Dropbox users must trust Dropbox with all
possible capabilities. Traditional cloud storage services such as
Dropbox, Drive, and OneDrive are thus classified as ``fully trusted''
services: service that require the highest possible level of user
trust. Such services, are thus also in a position to do the greatest
degree of damage to user privacy should a users trust in them as
faithful stewards of private data turn out to be misplaced.

The level of trust requested by tradition cloud storage servers
rightfully makes some user nervous or unwilling to use such
services. In response to such aversion, a number of systems have been
developed with the aim of overcoming third party trust challenges in
the storage space. These systems include ``end-to-end'' encrypted file
storage services such as Tresorit~\cite{tresorit}, or
SpiderOak~\cite{spideroak}. These systems aim to place limits on a
third party's ability to leverage the access (\emph{R}) capability
through the use of client-side encryption. Likewise, they aim to limit
third party access to the manipulation (\emph{W}) capability through
the use of client-side cryptographic
authentication.\footnote{E.g. asymmetric cryptographic signatures such
  as those provided by GnuPG~\cite{gnupg} or symmetric cryptographic
  message authentication codes (MACs) available via a variety of
  algorithms~\cite{dworkin2005, dworkin2007, dworkin2008}.}  In the
base case where a user merely wishes to store data on a single device
and not share it with others, these systems are fairly successful in
achieving their desired trust mitigations. In order to sync data
across multiple devices using such systems, a user must manually
provide some secret (e.g. a password, etc) on each device to secure
its operation. While potentially burdensome and inconvenient, this
practice is in line with these services trusted capabilities
mitigation since it does not require any additional third party trust.

The place where these systems falter at mitigating third party trust
is via their support for multi-user sharing and collaboration. Such
services tend to accomplish multi-user sharing by acting as a trusted
certificate authority (CA) in charge of issuing user
certificates.\footnote{A certificate is a combination of a user's
  public key and certain metadata signed by a trusted issuer. See for
  more information.}These certificates are then used with various
asymmetric cryptographic primitives) to exchange the necessary secrets
for bootstrapping sharing between users. Unfortunately, as a trusted
CA, these services are capable of issuing fraudulent user certificates
to themselves or other parties. This allows them to mount
man-in-the-middle (MitM) attacks on any user trying to share data by
impersonating the recipient of the shared data. This deficiency is
discussed in depth at~\cite{wilson2014}, and leads to a breakdown of
such services' claim that their users need not trust them, at least
when employing multi-user sharing. By mounting a MitM attack on a user
trying to share data with another user, such service providers can
regain the \emph{R} and \emph{W} capabilities they claim not to
have. Furthermore, these services do little to mitigate their access
to metadata (\emph{M} capability). Nor do they provide ways for users
to avoid data loss in the event that one of the services goes offline
or shuts down (\emph{S} capability).

``Secure'' cloud file storage service such as Tresorit do more to
minimize the required degree of third party trust then traditional
services such as Dropbox. In single-user scenarios, such services
succeed at reducing the degree of user trust from full (all four
capabilities) to partial (only requiring the \emph{S} and \emph{M}
capabilities). Yet, when implementing multi-user use cases, such
services fall back to requiring a more-or-less full degree of trust,
leaving much to be desired.

\subsubsection{Social Media}

Social media sites such as Facebook, Google Plus, etc have become
popular since the early 2000s. Such sites maintain a ``social-graph''
of connections between users, and facilitate communication and sharing
of pictures, events, and other data between users. Such sites are
generally ``free'' to users -- monetizing user data and interactions
for the purpose of selling targeted advertising. Given their ubiquity
in the modern Internet landscape, as well as their position as
ad-supported services, it is useful to evaluate the trust profile of
modern social media sites. Facebook is the largest social media site
today, serving over 1.5 billion users as of 2015~\cite{foster2014}. As
such, Facebook serves as an example of the variety of social media
sites available today.

In terms of capabilities, Facebook, like Dropbox and other traditional
cloud services, must be trusted with a full range of capabilities.
Facebook is responsible for faithfully storing user data such as
photos, videos, and messages. Facebook can access and read all data it
stores, and indeed relies on the ability to read such data as the
basis of their advertising-based business model. Facebook can
manipulate the data it stores, and routinely does so for the purpose
of curating user ``news feeds'' or even integrating user pictures into
targets ads~\cite{mashable-socialads}. Finally, Facebook is capable of
applying a range of meta-analytic techniques to acquire additional
data about users for the purpose of targeting both ads as well as
content from other users.

Other social media sites such as Google+~\cite{google-plus}
require similar levels of trust. And since all mainstream social media
services operate on add-supported business models, there are
business-related barriers to reducing this level of trust where doing
so would also reduce the level of access to user data. Thus, unlike in
the storage space, there are not really any options for ``secure''
social media platforms that specifically aim to minimize third party
trust.

\subsubsection{Communications}

Communication systems ranging from email and chat to voice and video
calling are another popular set of third party services. The privacy
and security of these systems are a matter of great public concern,
and indeed many of the current privacy and security related legal
battles revolve around the ability to communicates in a private and
secure manner (e.g.~\cite{apple-fbiletter, greenwald-prism,
  levsion-lavabit}). Communication systems range from traditional
communications services such as Gmail~\cite{google-gmail} to recent
privacy-enhancing services such as
TextSecure~\cite{otr-advanced-ratchet}.

Email services such as Gmail~\cite{google-gmail} or chat services such
as Hangouts~\cite{google-hangouts} represent a fairly traditional
approach to third party communication services. As was the case with
Dropbox and Facebook, users of such services must rely on the third
party service provider (in this case, Google) to properly store
(\emph{S} capability) their messages while the design of these systems
do nothing to prevent the service provider from accessing (\emph{R}
capability) or manipulating (\emph{W} capability) user
messages.\footnote{Similar to Facebook, many communication services
  are ad-based, and thus the service provider often relies on their
  ability to access user data as the basis of their business
  models.}Furthermore, since all communication flows through the
service provider's servers, these providers have access to a range of
potentially reveling meta-data about their users (\emph{M}
capability).

The need to place a high degree of trust in various third parties in
order to leverage digital communication services has long been a
concern. Indeed many early privacy-enhancing software projects,
including the venerable PGP~\cite{zimmermann-pgp10,
  zimmermann-pgpsource}, were created in response to the lack of
privacy inherent in most digital communication systems. Modern
implementation of such systems, such as those conforming to the
OpenPGP protocol~\cite{callas2007}, aim to reduce the amount users
must trust third party communication providers by adding end-to-end
encryption and cryptographic authentication support to traditional
digital communication mediums. The OpenPGP protocol can be applied
atop mail traversing traditional email systems such as Gmail, as well
as to messages traversing chat applications such as Hangouts. When
used with such services, OpenPGP provides a level of trust mitigation
above and beyond what is possible to achieve via the native services
themselves. In terms of trusted capabilities, a user employing PGP
atop a traditional third party cloud service such as Gmail minimizes
both the third party's access (via encryption) and manipulation (via
authentication) capabilities. In such a scenario, only the end users
involved in a given communication, and not any third party through
which that communication might pass, have access to the necessary
cryptographic keys required to read or alter the message. The third
party, however, can still capture metadata (\emph{M} capability) about
the communication since such metadata is outside of the scope of the
message content that PGP is capable of securing. The third party is
also still capable of dropping or deleting the communication all
together, and thus still possesses the \emph{S} capability.

Due to the numerous challenges and deficiencies associated with using
OpenPGP-based systems~\cite{green-pgp, borisov2004, whitten1999},
developers have created a number of alternate secure communication
protocols. These protocols aim to provide forward-secrecy, metadata
privacy, deniability, contact authentication, and message encryption
and authentication for (primarily) real-time communication such as
instant messaging and chat systems. Examples of such protocol include
OTR~\cite{otr-v3} and OTR-derived protocols like
TextSecure~\cite{otr-advanced-ratchet}. The TextSecure protocol is
used by several apps such Open Whisper System's
Signal~\cite{openwhisper} and WhatsApp~\cite{whatsapp}. TextSecure
uses various forms of asymmetric cryptography to provide users with
end-to-end encrypted and authenticated individual and group messaging
capabilities. Use of TextSecure denies any third party through which
TextSecure messages might pass (including the TextSecure server
itself) the access or manipulation capabilities. Furthermore,
TextSecure makes efforts to secure metadata from third party actors,
including the TextSecure server provider itself. These efforts curtail
a third party's ability to analyze message metadata. It is still
possible for a network-level adversary or the TextSecure server
provider to discover the raw network (e.g. IP) endpoints involved in a
TextSecure exchange, but higher level details are not
available.\footnote{It is possible to couple TextSecure with existing
  network anonymity systems such as Tor~\cite{dingledine2004} to
  mitigate such network-level
  meta-analysis~\cite{intercept-chatting}.}TextSecure users are still
dependent on a third party to operate a TextSecure server in order to
communicate in the first place (e.g. it is not a distributed
protocol), but beyond this ``storage''-like capability, TextSecure
grants no other capabilities to any third party.

Following the trend set by traditional cloud services such as Dropbox
and Facebook, traditional communication systems such as Gmail (and
email in general) or Hangouts (and related chat systems) require user
to place a high degree of trust in the corresponding third party
service providers. Overlay privacy-enhancing systems such as those
implementation the OpenPGP protocol allow users to reduce this level
of trust by leveraging client-side cryptography to prevent third
parties from levering the access (\emph{R}) or manipulation (\emph{W})
capabilities. Modern full-stack privacy-focused communication
protocols such as those implementing flavors of the OTR protocol take
these privacy-preserving cryptography techniques a step further by
providing primitives for limiting third party metadata access in
addition to protecting user data from third party access or
manipulation.

\subsubsection{Password Managers}

Password management programs are commonly used by end users wishing to
both manage and increase the security of the credentials they use to
access web-based resources. Such programs are useful for helping end
users remember passwords, and by extension, for encouraging users to
use stronger (i.e. longer and/or more random)
passwords~\cite{brodkin-passman, krebs-passwords,
  schneier-passwords}. Since Password managers potentially have access
to user credentials -- access to which would allow an adversary to
access a range of user data, including many of the cloud services
previously discussed, it is worth evaluating the trust user much place
in such services.

LastPass~\cite{lastpass} is one of the most popular cloud-based
password managers. LastPass operates by storing encrypted user
passwords on a LastPass-controlled server. Passwords are encrypted on
the client and only encrypted passwords are sent to LastPass. Each
password is encrypted using a key derived from a user-supplied
``master'' password. LastPass never stores this master password
directly, making it difficult for them to derive the key necessary to
decrypt the encrypted data they store. Thus, LastPass intentionally
limits its ``access'' (\emph{R} capability) to user
passwords. LastPass does not, however, appear to perform any kind of
cryptographic authentication on the data it stores, meaning it still
has full capabilities over the ``manipulation'' (\emph{W})
capability.\footnote{Such a lack of client-side cryptographic
  protections against modification leaves the door open to a range of
  potential attacks on LastPass's client side encryption as per the
  ``cryptographic doom'' principle~\cite{marlinspike-doom}.}Similarly,
LastPass is fully responsible for faithfully storing user data and has
full access to all user metadata associated with any stored
password. Thus LastPass, requires users to trust it with three of the
possible four capabilities -- less trust than cloud services such as
Facebook or Dropbox, but more than is strictly necessary to perform
its password storage duties.

Other open-source password managers such as KeePass~\cite{keepass},
Password Safe~\cite{passwordsafe}, or Pass~\cite{pass} exist with the
aim toward reducing the need to trust one or more third parties. Such
systems accomplish this by either requiring no third party support at
all (e.g. a purely local password manager)\footnote{Such purely
  client-side solutions limit third party trust, but do so at the
  expense of usability -- e.g. such solutions rarely provide users
  with the ability to access their passwords from multiple devices or
  to share passwords with colleagues.}or by allowing the user to
decouple encryption and authentication operation from optional third
party backend data storage and sync providers such as Dropbox. In
addition to liming third party access capability via encryption, such
services often aim to limit both manipulation and metadata
capabilities via the use of client-side cryptography.

\subsection{Violation Examples}
\label{sec:analysis:violations}

Beyond the capabilities with with users must trust modern cloud
services lie the analysis of the likelihood that such trust will be
violated. In this section, I present an analysis of the factors and
incentivize and disincentive certain classes of trust violation (as
outlined in \S~\ref{sec:model}). I also provide examples of specific
trust violation events that have occurred over the previous ten years.

\subsubsection{Implicit Violations}

Implicit trust violations represent the most direct form of trust
violation. Implicit violation occur when a trust party intentionally
misuses a trusted capability in a eminent the user did not intend. As
the most direct for of violation, implicit violates are also present
the simplest analysis of incentives and disincentive regarding such
violations.

One of the clearest potential incentives for companies to commit
implicit trust violation comes via advertising-supported business
models employed by many cloud services. In these models, the user is
provided with access to a cloud service for ``free''. The service
provider monetizes their service either by selling advertising space
on the service directly or by collecting and selling user data to
third party advertising firms. In contrast to more traditional
mass-media based advertising schemes, cloud services are often
designed as platforms for highly targeted advertising. That is, cloud
services can leverage the vast amount of user data to which they have
access as a mechanism for building detailed dossiers on each user and
using these dossiers as the basis for serving personally tailored
adds. Advertisers are generally willing to pay higher prices for more
carefully tag red ads, incentivizing cloud providers to harvest user
data is pursuit of such targeting.

While such advertising practices do not inherently represents an
implicit trust violation, they do setup a series of perverse
incentives where companies can benefit by leveraging the access
(\emph{R}) capability to harvest user data. Most ad-support cloud
service require the user to agree to a terms of service that grants
the service provider the right to harvest user data for advertising
purposes. But it's well known that few, if any users, actually read
such terms, leading to situations where users are often surprised by
the way in which their data is used~\cite{hern2015}. Thus, while the
user may have technically ``agreed'' to certain advertising practices,
it is reasonable to still fault a service provider with having
committed an implicit trust violation in situations where their use of
user data deviations from that which the user would generally expect.

Target provides an example of an implicit trust violation triggered by
ad-motivated misuses of access to user data. In 2012, it became public
that Target had developed a statistical system for predicting if its
shoppers were pregnant based on the kind of items they bought. Target
would then leverage this data to send customers coupons optimized for
pregnant individuals. In one case, this practice lead to the outing of
a pregnant teenager to her previously unaware
father~\cite{hill2012}. Clearly such outcomes are not within the realm
of what most shoppers expect when purchasing items at Target. Facebook
committed a similar ad-motivated implicit trust violations when it
begin to incorporate user-provided images into the ads it served to
other users~\cite{mashable-socialads}. These actions caught many users
by sup rise as one does not normally expect their personal photos to
be re-purposed for the purpose of endorsing third party products.

Not all implicit violations are tied to the kinds of preference
incentives user-data driven advertising often elicits. Sometimes third
parties just make poor decisions about the manner in which to use the
capabilities a user has granted them. One of the more infamous
examples of such misuse comes from Facebook's ability to manipulate
(\emph{W} capability) user news feeds. In 2014, it came to light that
Facebook had engaged in research that involved manipulating what users
saw in their news feeds in order to study the effects of one user's
emotions on other users~\cite{goel2014}. The ``emotional contagion''
study was performed on $\approx700$ users without their knowledge or
consent. Facebook misused the trust placed in it by its users to
faithfully curates their news feeds to instead manipulate these feeds
in unforeseen and potentially behavior altering ways.

Some cloud companies rely on charging their users for access to a
given service, and are thus particular disincentivized from committing
implicit violations -- the revelations of which might harm their
business prospects. But such companies are not immune to committing
implicit violations. For example, in 2014, ride-share app
Uber~\cite{uber} made headlines when it used the travel history of a
number of its more prominent users to display a live user-location map
at a launch party~\cite{sims2014}. This map allowed party guests to
track these users in real time -- an outcome the average Uber user
certainly does not expect. Similarly, Uber also used stored user
travel history to compose a blog post detailing its ability to detect
a given user's proclivity for ``one night
stands''~\cite{pagliery2014}. In both cases, Uber committed implicit
trust violations by leveraging data it had about users in manners
users did not approve of or intend.

\subsection{Compelled Violations}

While implicit trust violations are perhaps the most egregious form of
violations, they are certainly not the most pervasive. Instead, that
honor likely falls to compelled violations. As discussed in
\S~\ref{sec:model}, compelled violations occur when an entity other
than the third party the user is trusting forces the third party to
manipulate or provide user data in a manner of which the user does not
approve. The most common form of compelled violation comes via
government search and seizure powers. In the United States, such
powers are often exercised via a variety of forms including subpoenas
issued under the Third Party Doctrine~\cite{thompson-thirdparty},
probable cause search warrants~\cite{us-constitution-amend4}, National
Security Letters~\cite{fbi-nsl}, and Foreign Intelligence Surveillance
Court (FISC)~\cite{fisc} orders.

\begin{table}[thb]
  \footnotesize
  \centering
  \tabulinesep = 3pt
  \begin{tabu} to \textwidth
    { | X[2,c,m]
      | X[1,c,m]
      | X[1,c,m]
      | X[1,c,m]
      | X[1,c,m]
      | X[1,c,m]
      | }
    \hline
    \textbf{Company}
    & \textbf{2011}
    & \textbf{2012}
    & \textbf{2013}
    & \textbf{2014}
    & \textbf{2015}
    \\ \hline 
    Facebook
    & Unknown
    & Unknown
    & 19292
    & 23666
    & Incomplete
    \\ \hline
    Google
    & 11413
    & 14612
    & 17749
    & 18300
    & Incomplete
    \\ \hline
    Twitter
    & Unknown
    & 1072
    & 1179
    & 2203
    & 4060
    \\ \hline 
    Dropbox
    & Unknown
    & 71
    & 198
    & 404
    & Incomplete
    \\ \hline 
    Amazon
    & Unknown
    & Unknown
    & Unknown
    & Unknown
    & Incomplete
    \\ \hline 
 \end{tabu}
  \caption{ U.S. Government Data Requests Resulting in User Data Being
    Provided By Year }
  \label{tab:analysis:violations:reports}
\end{table}

The scope of compelled violation can be partially evaluated by
studying the transparency reports published by many cloud
companies. Companies such as Dropbox~\cite{dropbox-transparency},
Amazon~\cite{amazon-transparency},
Facebook~\cite{facebook-transparency},
Google~\cite{google-transparency}, and
Twitter~\cite{twitter-transparency} all published bi-annual
transparency reports detailing the number of type of data requests
they received as well as the frequency at which they turn over user
data or metadata in response to these requests. While the requests
outlined in these reports are generally lawful, and in some cases, are
likely important for protecting the safety of the public, turning over
data in response to such requests without user permissions represents
a compelled violation since users do not generally intend for third
parties to provide their data to government
actors. Table~\ref{tab:analysis:violations:reports} shows the number
of instances in which several major third parity service providers
were compelled to turn over user data or metadata over the previous
five years. As shown, the largest third party service providers turn
over user data on the order of 10s of thousands of times per
year. Furthermore, the number of compelled violations committed each
year has steadily risen from year to year.

The high numbers of compelled violations that occur each year likely
represent a fairly significant increase in the amount of user data
being provided to government actors relative to pre-cloud computing
times. While it is possible that governments would serve the same
number of data request on individual users were we to live in a world
where most Eur data was individually stored instead of held by third
parties, it seems unlikely that this would be the case. The
concentration of user data in a handful of third parties greatly
reduces the effort required by government actors to request access to
such data. Furthermore, while certain types of compelled legal orders,
e.g. probable cause warrants, could in fact be served on individuals
instead of third parties, other legal orders, e.g.  subpoenas served
under the third party doctrine, would not be legally valid if served
on an individual.

Beyond the kinds of direct requests for user data discussed in
published transparency reports, there are also several notable
examples of governments seeking to compel individual third parties to
modify their services in order to enable compelled access to user
data. Lavabit provides an example of one such case from 2013. Lavabit
was a private email service with 400,000 users premised on the idea
that popular free email services such as Gmail lacked adequate
security and privacy guarantees. In August 2013 Lavabit shuttered its
service in response to a U.S. government subpoena requiring Lavabit to
turn over all of its encrypted user traffic as well as the associated
SSL encryption keys necessary to decrypt it~\cite{lavabit,
  levsion-lavabit}. After a legal fight, Lavabit founder Ladar Levison
was forced to disclose the encryption keys protecting his service.

Similarly, the recent (and ongoing) Apple v. FBI fight illustrates the
lengths governments might be willing to go to to ensure they can
compel access to user data.  In response to the 2015 San Bernardino
shootings, the FBI attempted to compel Apple to help it decrypt one of
the shooters' iPhone~\cite{ars-cookvfbi}. The form of encryption Apple
uses to protect the iPhone involves a hardware-linked encryption key
that can not be easily extracted from the phone, limiting out-of-band
cracking opportunities. Furthermore, this key can not be used on the
phone without a user-provided passcode. By default, Apple limits the
number of guesses a user may make at this passcode and throttles the
speed at which a user may guess passcodes. The FBI wished to compel
Apple to update the software on the iPhone so that they could try to
guess an unlimited number of passcodes at a high rate of
speed~\cite{eff-applecrypto}. Apple was disinclined to acquiesce to
this request~\cite{apple-fbiletter}. The case wound up being dropped
by the FBI after they were able to leverage an undisclosed security
vulnerability to bypass Apple's passcode guessing limits
directly~\cite{ars-fbi-greyhats, ars-fbi-breakthrough}. As in the
Lavabit case, this case demonstrates the government's interest in
compelling companies to assist them in accessing private user data,
even going so far as to potentially require companies to avoid the use
of certain forms of encryption or security-enhancing features that
would make such assistance difficult or impossible to provide.

Not all compelled trust violations inherently involve government
requests for user data. Sometimes third parties may be compelled to
turn over user data due to business circumstances. In particular, it
is not unusual for user data to be bought or sold in the event that a
third party goes bankrupt~\cite{singer2015, solove2015,
  nguyen2004}. Since the sale of such data in a bankrupt situation or
acquisition is often beyond the direct control of the third party,
such data transfers represent a compelled trust violation (as opposed
to the implicit trust violation that would occur if a third party
willfully sold or shared user data in a manner the user does to expect
their data to be sold or shared).

Regardless of the method in which they occur, compelled violations are
one of the most common form of third party trust violations.

\subsubsection{Unintentional Violations}

As mentioned in \S~\ref{sec:model}, unintentional violations occur
when a third party violates a user trust in a manner that they neither
intended or were forced to do. Unintentional violations can be broadly
sorted into two subcategories: external and internal
violations. External violations are caused by an external actor
(e.g. an adversarial attacker) leveraging a third parties capabilities
to cause a trust violation. Internal violations are caused by mistakes
within the third party (e.g. a coding error) leading to a trust
violation. Often, internal violations beget external violations -- for
example, a security bug caused by a programming mistake could open the
door to external attacks that leverage the bug to expose user data.

There have been a number of notable internal unintentional trust
violations committed by third parties over the past ten years. For
example, in 2011 Dropbox introduced a bug into their authentication
system that allowed anyone to log into the service using any password
for a five hour period~\cite{dropbox-authbug}. While Dropbox certainly
did not intend to share many users files with the entire world, they
unintentionally did so via a coding error. In some cases, internal
violations occur due to factors beyond the third parties direct
control. For example, third parties are susceptible to a range of
software flaws in externally maintained libraries they rely
on. Prominent examples of such flaws include
Heartbleed~\cite{heartbleed}, a flaw in OpenSSL~\cite{openssl} that
allowed attackers to steal private data from many secure servers, and
Shellshock~\cite{symantec-shellshock}, a GNU bash~\cite{gnu-bash} flaw
that allowed user to execute arbitrary code on many web servers. Both
flaws were widespread and effected large swaths of third party sites
and services, potentially exposing the user of such services to data
exfiltration or manipulation (e.g. Violations of third party \emph{R}
or \emph{W} capabilities).

While open source code such as OpenSSL and Bash have been the source
of several trust-violating software bugs, it is also possible for open
source approaches to help remedy such bugs. While bugs like Heartbleed
or Shellshock demonstrate that having publicly reviewable code is not
sufficient for eliminating bugs, these bugs also demonstrate effective
disclosure and patching processes inherent to one source code. Had
similar bugs been discovered internally in non open source code bases,
it is possible they would have gone unreported and no one would be the
wiser. The difficulty of hiding bugs or ignoring publicly disclosed
bugs in open source code bases has led a number of security and
privacy enhancing software projects to specifically leverage open
source models. Projects such as GnuPG~\cite{gnupg},
Signal~\cite{openwhisper}, and End to End~\cite{google-endtoend,
  yahoo-endtoend} are tout their open source nature as a mechanism for
reading the likelihood of moth unintentional bugs as well as
intentional back doors. Indeed, such practices can be viewed as a form
of trust mitigation since they allow the user to reduce the trust they
must place in third party provided code itself in favor of using
publicly reviewable code. While open source implementations alone do
not guarantee the lack of such bugs~\cite{frosch2014} or
back doors~\cite{thompson1984}, it does help to maximize the number of
eye on the code base, making such violation harder to hide.

Internal unintentional violations often pave the way of external
unintentional violations. Negligence or incompetence on the apart of a
third party might open the door for an external attack. Take, for
example, the recent OPM data breach. In 2015, the U.S. Office of
Personnel Management (OPM) announced that their systems had been
breached, exposing the personal data of essentially anyone who had
held or currently holds a U.S. Government security
clearance~\cite{ars-opmhack, opm-cybersecurityincidents}. This breach,
in addition to having high strategic value to foreign attackers,
reveled sensitive personnel data of a huge number of U.S.  government
employees and contractors. This leak was largely due to the use of old
and outdated storage and security systems employed by the OPM. While
such usage did not necessarily directly result in the lack of
sensitive user data, it certainly made it far easier for an attacker
to break in and steal such data. It is likelihood that a similar
situation occurred in early 2015when several major U.S. health
insurance companies were subject to attacks that breached their user
records, allowing the release of personal, financial, and medical
information on millions of users~\cite{krebs-anthem,
  krebs-premera}. While the details of the breaches were not made
public, it is likelihood that mistakes on the apart of the third party
storing the data paved the way for external actors to steal user
data. Recent investigations of common data breach causes list errors
on the part of third party data stewards as the leading cause of
breaches~\cite{verizon-2016breach, gallagher-blame}.

Some external unintentional violations occur not due to the fault of the
third parties, but due to a fault of the user themselves. The most
common example of such failures involve the use of weak passwords by
users to protect their accounts on third-party services.  Dropbox has
been the target of various external trust violations built around
advisories who obtain common and exploit user
passwords~\cite{dropbox-passwords}. While it is tempting to not
attribute these faults to third parties themselves, third party service
providers must shoulder at least some of the blame for allowing users
to utilize weak credentials or similar error-prone authentication
mechanisms. For example in 2014, a number of celebrity users of
Apple's iCloud data storage service~\cite{apple-icloud} were subject
to a public release of personal photos they had stored with the
service. This leak was the result of a targeted attack on the
corresponding users' passwords and iCloud
accounts~\cite{apple-icloudleak}. These attacks appear to have been
propagated over several months prior to the public release. While this
leak was not a result of an overt flaw in Apple's iCloud system, the
weak default security requirements for iCloud accounts made it
relatively simple for attackers to compromise such accounts and steal
data.

Finally, some external unintentional violations occur due to an
adversaries use of techniques that many third parities could not be
reasonably expected to defend against. Nation-state level outsider
attacks generally fall into this category. While governments are
generally able to access user data via by compelling third parties to
turn it over, in some cases, they prefer to attack the third party
directly, triggering an unintentional outsider type violation. For
example, the NSA PRISM program was/is a Foreign Intelligence
Surveillance Court (FISC)~\cite{fisc} approved system for compelling
service providers to provide user data to the
government~\cite{greenwald-prism}. It is believed to be one of the
largest programs used by the government to extract user data from
various cloud-based services (e.g. Google, Yahoo, Microsoft,
etc). Similarly, MUSCULAR was/is a joint NSA and U.K. Government
Communication Headquarters (GCHQ) effort to intercept and monitor
traffic traversing Google's and Yahoo's inter-datacenter
networks~\cite{gellman-muscular}. Prior to MUSCULAR's disclosure, this
intra-datacenter traffic was not generally encrypted, and thus was an
ideal point for the government to intercept and monitor user data,
especially in cases where the government is able to utilities such
far-reaching technique as tapping undersea communication tables or
forcing access to ``secure'' Internet exchange facilities. These types
of violations are often bootstrapped either via internal unintentional
violations (e.g. exploitable bugs in a crypto algorithm used by a
third party) or via compelled orders (e.g. such as those granting
government access to the underlying infrastructure mounting such
attacks).

\subsubsection{Collusion Violations}

Collusion violations occur when multiple third parties work in concert
to leverage or misuses capabilities each in manners that would not be
possible for each individually to do. Inherent to the notion of
collusion violations is the notion of trust separation -- e.g. the
ability to spread trusted capabilities across multiple third parties,
reducing the amount any individual third party must be trusted in the
process. This fact makes examples of real world collusion violations
harder to come by by nature of the fact that very few deployed systems
require, or even allow, users to distribute trust in these manners.

Still, one can imagine how certain collusion violations might
occur. For example, a password manager provider such as LastPass could
collude with a mobile keyboard provider such as Swype~\cite{swype} for
the purpose of capturing a user's ``master'' LastPass encryption
password and suing it to decrypt the users passwords. Normally,
LastPass lacks access to the plain-text variants of the user passwords
it stores since these are encrypted on the client's device prior to
being sent to the LastPass servers. Similarly, mobile keyboard
software such as Swype does not normally posses access to a LastPass
user's passwords since these are encrypted and stored via the separate
LastPass app. Thus, neither Swype nor LastPass can individually access
a user's password data. Swype does, however, have the ability to
modify their keyboard software to record a user's typed input and
report it back to a central server. Thus, Swype could collude with
LastPass to capture and provide LastPass with user master passwords
which LastPass could then use to gain the ability to read stored
passwords even though the user attempted to limit such access via the
sue of client-side encryption.\footnote{This example is a bit
  contrived for the purposes of demonstration. In reality, if LastPass
  wished to capture user Master password input, or otherwise decrypt
  user passwords, they could likely just modify the LastPass client
  app directly to either record all user input or to add a backdoor to
  the encryption mechanism. They would not inherently need to involve
  an additional third party such as Swype in order to mount such an
  attack.}

While real-world collusion violations appear to be relatively rare
today as a side effect of the singular nature of most third-party
trust arrangements, this may change in the
future. \S~\ref{sec:mitigation} discusses techniques for reducing
trust in singular third parties, the side effects of which may be a
reduction in the kinds of violations that are common today, but an
increase in the potential for compelled violations.

\subsection{Overall Risk}

ToDo

%%  LocalWords:  OneDrive FISC Tresorit SpiderOak MACs MitM CALEA OTR
%%  LocalWords:  TextSecure WhatsApp LastPass LastPass's KeePass Uber
%%  LocalWords:  Lavabit Ladar Levison Bernardino Heartbleed OpenSSL
%%  LocalWords:  Shellshock OPM iCloud NSA GCHQ MUSCULAR's Swype
